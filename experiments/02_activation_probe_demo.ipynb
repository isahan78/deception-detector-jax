{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Probing Demo\n",
    "\n",
    "This notebook demonstrates how to use linear probes to decode hidden variables from transformer activations.\n",
    "\n",
    "**Goal**: Train a linear classifier to predict whether an example is \"deceptive\" based solely on internal activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "\n",
    "from deception_detector_jax.config import ModelConfig, DatasetConfig\n",
    "from deception_detector_jax.models.tiny_transformer import init_model\n",
    "from deception_detector_jax.data.deception_tasks import generate_task\n",
    "from deception_detector_jax.interp.activation_cache import run_with_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Dataset with Hidden Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Hidden Check task\n",
    "data_config = DatasetConfig(\n",
    "    task_name=\"hidden_check\",\n",
    "    num_train=2000,\n",
    "    deception_rate=0.3,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "data = generate_task(\"hidden_check\", data_config, 2000)\n",
    "\n",
    "print(f\"Generated {len(data['input_ids'])} examples\")\n",
    "print(f\"Forbidden rate: {data['forbidden'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Model (Pretrained or Random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model_config = ModelConfig(\n",
    "    seq_len=32,\n",
    "    d_model=64,\n",
    "    n_heads=4,\n",
    "    n_layers=2,\n",
    "    vocab_size=128,\n",
    "    collect_intermediates=True\n",
    ")\n",
    "\n",
    "rng = jax.random.PRNGKey(0)\n",
    "model, params = init_model(model_config, rng)\n",
    "\n",
    "print(\"Model initialized!\")\n",
    "# TODO: Load pretrained parameters if available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract Activations from All Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run forward pass on all examples\n",
    "inputs = jnp.array(data['input_ids'])\n",
    "logits, cache = run_with_cache(model, params, inputs)\n",
    "\n",
    "print(f\"Logits shape: {logits.shape}\")\n",
    "print(f\"Cache collected for {len(data['input_ids'])} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract MLP activations from last layer\n",
    "layer_idx = model_config.n_layers - 1\n",
    "mlp_acts = cache.get_mlp_activations(layer_idx)\n",
    "\n",
    "if 'mlp_post_act' in mlp_acts:\n",
    "    activations = np.array(mlp_acts['mlp_post_act'])\n",
    "    print(f\"MLP activations shape: {activations.shape}\")\n",
    "    \n",
    "    # Average over sequence dimension\n",
    "    activations_pooled = activations.mean(axis=1)\n",
    "    print(f\"Pooled activations shape: {activations_pooled.shape}\")\n",
    "else:\n",
    "    print(\"No MLP activations found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Linear Probe to Detect Forbidden Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for probe\n",
    "X = activations_pooled\n",
    "y = data['forbidden']\n",
    "\n",
    "# Split train/test\n",
    "n_train = int(0.8 * len(X))\n",
    "X_train, X_test = X[:n_train], X[n_train:]\n",
    "y_train, y_test = y[:n_train], y[n_train:]\n",
    "\n",
    "print(f\"Training set: {len(X_train)} examples\")\n",
    "print(f\"Test set: {len(X_test)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train logistic regression probe\n",
    "probe = LogisticRegression(max_iter=1000, random_state=42)\n",
    "probe.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = probe.predict(X_test)\n",
    "y_proba = probe.predict_proba(X_test)[:, 1]\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "print(f\"Probe Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Probe AUC: {auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Probe Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, linewidth=2, label=f'ROC (AUC={auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve: Forbidden Case Detection')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze Probe Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get probe weights\n",
    "weights = probe.coef_[0]\n",
    "\n",
    "# Plot weight magnitudes\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.bar(range(len(weights)), np.abs(weights))\n",
    "plt.xlabel('Feature Index (MLP Neuron)')\n",
    "plt.ylabel('Absolute Weight')\n",
    "plt.title('Probe Weight Magnitudes (Which neurons are most informative?)')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.show()\n",
    "\n",
    "# Find most important features\n",
    "top_k = 10\n",
    "top_indices = np.argsort(np.abs(weights))[-top_k:][::-1]\n",
    "print(f\"\\nTop {top_k} most important features:\")\n",
    "for i, idx in enumerate(top_indices, 1):\n",
    "    print(f\"  {i}. Feature {idx}: weight = {weights[idx]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Interpretation\n",
    "\n",
    "If the probe achieves high accuracy (>0.7), it means:\n",
    "- The model's activations **encode** the hidden forbidden condition\n",
    "- This information is linearly accessible\n",
    "- We can potentially intervene on these features to change behavior\n",
    "\n",
    "## TODO:\n",
    "- Probe different layers to see where information emerges\n",
    "- Compare attention vs MLP activations\n",
    "- Visualize activation patterns for high-weight neurons"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
